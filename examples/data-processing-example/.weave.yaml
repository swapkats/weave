version: "1.0"

# Data Processing Pipeline
# ETL workflow with analysis and visualization

env:
  DEFAULT_MODEL: "gpt-4"
  DATA_FORMAT: "json"

tools:
  database_connector:
    description: "Connect to databases"
    parameters:
      type: "object"
      properties:
        connection_string:
          type: "string"
        query:
          type: "string"
      required: ["connection_string", "query"]

  file_reader:
    description: "Read data files"
    parameters:
      type: "object"
      properties:
        file_path:
          type: "string"
        format:
          type: "string"
      required: ["file_path"]

  data_validator:
    description: "Validate data quality"
    parameters:
      type: "object"
      properties:
        data:
          type: "object"
        schema:
          type: "object"
      required: ["data"]

agents:
  extractor:
    model: "${DEFAULT_MODEL}"
    description: "Extract data from sources"
    tools: [database_connector, file_reader]
    config:
      temperature: 0.1
      max_tokens: 2000
    system_prompt: |
      Extract data from sources:
      - Connect to data sources
      - Execute queries efficiently
      - Handle pagination and limits
      - Log extraction metrics
      - Output in ${DATA_FORMAT} format
    outputs: "raw_data"

  transformer:
    model: "${DEFAULT_MODEL}"
    description: "Transform and clean data"
    tools: [data_validator]
    inputs: "extractor"
    config:
      temperature: 0.2
      max_tokens: 3000
    system_prompt: |
      Transform and clean data:
      - Remove duplicates and nulls
      - Standardize formats
      - Validate against schema
      - Handle missing values
      - Apply business rules
    outputs: "clean_data"

  analyzer:
    model: "${DEFAULT_MODEL}"
    description: "Analyze data for insights"
    tools: []
    inputs: "transformer"
    config:
      temperature: 0.3
      max_tokens: 2000
    system_prompt: |
      Analyze data for insights:
      - Calculate key metrics
      - Identify trends and patterns
      - Detect anomalies
      - Generate statistics
      - Highlight actionable insights
    outputs: "analysis"

  reporter:
    model: "${DEFAULT_MODEL}"
    description: "Generate reports and visualizations"
    tools: []
    inputs: "analyzer"
    config:
      temperature: 0.5
      max_tokens: 2000
    system_prompt: |
      Create data report:
      - Summarize key findings
      - Suggest visualizations
      - Highlight important trends
      - Provide recommendations
      - Format for stakeholders
    outputs: "report"

weaves:
  etl_pipeline:
    description: "Complete ETL with analysis"
    agents:
      - extractor
      - transformer
      - analyzer
      - reporter

  data_quality:
    description: "Focus on data quality"
    agents:
      - extractor
      - transformer
